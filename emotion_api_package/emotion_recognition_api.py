#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DAiSEEÊÉÖÊÑüËØÜÂà´APIÊé•Âè£
ËæìÂá∫Âõõ‰∏™ÊÉÖÊÑüÊ†áÁ≠æ„ÄÅÂç†ÊØîÂíåÊúÄÁªàÊâìÂàÜ
"""

import os
import torch
import torch.nn as nn
import numpy as np
import cv2
from torchvision import transforms
from flask import Flask, request, jsonify
import base64
import tempfile
import json
from datetime import datetime

# ËÆæÁΩÆËÆæÂ§á
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class FastEmotionModel(nn.Module):
    """Âø´ÈÄüËΩªÈáèÂåñÊÉÖÊÑüËØÜÂà´Ê®°Âûã - Ê≠£Á°ÆÊû∂ÊûÑ"""
    
    def __init__(self, sequence_length=8, num_classes=4):
        super(FastEmotionModel, self).__init__()
        
        # ËΩªÈáèÂåñCNNÁâπÂæÅÊèêÂèñÂô®
        self.conv_layers = nn.Sequential(
            # Block 1 - Âø´ÈÄü‰∏ãÈááÊ†∑
            nn.Conv2d(3, 32, kernel_size=5, stride=2, padding=2),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),  # 128 -> 32
            
            # Block 2 
            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),  # 32 -> 16
            
            # Block 3 - ÊúÄÁªàÁâπÂæÅ
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool2d((4, 4))  # 16 -> 4
        )
        
        # ÁÆÄÂåñÁöÑLSTM
        self.lstm = nn.LSTM(
            input_size=128 * 16,
            hidden_size=64,  # Êõ¥Â∞èÁöÑÈöêËóèÂ±Ç
            num_layers=1,
            batch_first=True
        )
        
        # ÊûÅÁÆÄÂàÜÁ±ªÂô®
        self.classifier = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(64, 32),
            nn.ReLU(inplace=True),
            nn.Linear(32, num_classes)
        )
        
        self._initialize_weights()
    
    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        batch_size, seq_len, c, h, w = x.size()
        
        # CNNÁâπÂæÅÊèêÂèñ
        x = x.view(batch_size * seq_len, c, h, w)
        features = self.conv_layers(x)  # (batch*seq, 128, 4, 4)
        features = features.view(features.size(0), -1)  # (batch*seq, 2048)
        
        # ÈáçÂ°ë‰∏∫Â∫èÂàó
        features = features.view(batch_size, seq_len, -1)  # (batch, seq, 2048)
        
        # LSTM - Âè™ÂèñÊúÄÂêé‰∏Ä‰∏™ËæìÂá∫
        lstm_out, (hidden, _) = self.lstm(features)
        last_output = hidden[-1]  # ÂèñÊúÄÂêé‰∏ÄÂ±ÇÁöÑÈöêËóèÁä∂ÊÄÅ (batch, 64)
        
        # ÂàÜÁ±ª
        output = self.classifier(last_output)
        
        return output

class EmotionRecognitionAPI:
    """ÊÉÖÊÑüËØÜÂà´APIÁ±ª"""
    
    def __init__(self, model_path='best_fast_daisee_model.pth'):
        self.device = device
        self.emotion_names = ['Boredom', 'Engagement', 'Confusion', 'Frustration']
        self.emotion_weights = {
            'Boredom': -1.0,      # Êó†ËÅäÊòØË¥üÈù¢ÊÉÖÁª™
            'Engagement': 1.0,    # ÂèÇ‰∏éÊòØÊ≠£Èù¢ÊÉÖÁª™
            'Confusion': -0.5,    # Âõ∞ÊÉëÊòØËΩªÂæÆË¥üÈù¢
            'Frustration': -1.5   # Êå´ÊäòÊòØÂº∫ÁÉàË¥üÈù¢
        }
        
        # Âä†ËΩΩÊ®°Âûã
        self.model = self._load_model(model_path)
        
        # Êï∞ÊçÆÈ¢ÑÂ§ÑÁêÜ
        self.transforms = transforms.Compose([
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        print(f"üéØ ÊÉÖÊÑüËØÜÂà´APIÂàùÂßãÂåñÂÆåÊàê")
        print(f"   ËÆæÂ§á: {self.device}")
        print(f"   Ê®°ÂûãË∑ØÂæÑ: {model_path}")
    
    def _load_model(self, model_path):
        """Âä†ËΩΩËÆ≠ÁªÉÂ•ΩÁöÑÊ®°Âûã"""
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"Ê®°ÂûãÊñá‰ª∂‰∏çÂ≠òÂú®: {model_path}")
        
        # Âä†ËΩΩÊ£ÄÊü•ÁÇπ
        checkpoint = torch.load(model_path, map_location=self.device)
        
        # È™åËØÅÊ®°ÂûãÊòØÂê¶Â∑≤ËÆ≠ÁªÉ
        if 'val_mae' not in checkpoint:
            raise ValueError("Ê®°ÂûãÊñá‰ª∂Êó†ÊïàÔºöÁº∫Â∞ëÈ™åËØÅ‰ø°ÊÅØ")
        
        print(f"üìä Ê®°Âûã‰ø°ÊÅØ:")
        print(f"   ËÆ≠ÁªÉËΩÆÊï∞: {checkpoint.get('epoch', 'N/A')}")
        print(f"   È™åËØÅMAE: {checkpoint.get('val_mae', 'N/A'):.4f}")
        
        # ÂàõÂª∫Ê®°ÂûãÂÆû‰æã
        model = FastEmotionModel().to(self.device)
        
        # Âä†ËΩΩÊùÉÈáç
        try:
            model.load_state_dict(checkpoint['model_state_dict'])
            print(f"‚úÖ Ê®°ÂûãÊùÉÈáçÂä†ËΩΩÊàêÂäü")
        except Exception as e:
            raise RuntimeError(f"Ê®°ÂûãÊùÉÈáçÂä†ËΩΩÂ§±Ë¥•: {e}")
        
        model.eval()
        
        # È™åËØÅÊ®°ÂûãÊùÉÈáçÈùûÈöèÊú∫
        self._verify_model_weights(model, checkpoint)
        
        return model
    
    def _verify_model_weights(self, model, checkpoint):
        """È™åËØÅÊ®°ÂûãÊùÉÈáçÊòØÂê¶Â∑≤ËÆ≠ÁªÉÔºàÈùûÈöèÊú∫Ôºâ"""
        # ÂàõÂª∫ÈöèÊú∫Ê®°ÂûãÂØπÊØî
        random_model = FastEmotionModel().to(self.device)
        
        # ÊØîËæÉÊùÉÈáçÂ∑ÆÂºÇ
        trained_params = []
        random_params = []
        
        for (name1, param1), (name2, param2) in zip(model.named_parameters(), random_model.named_parameters()):
            if param1.requires_grad:
                trained_params.append(param1.detach().cpu().numpy().flatten())
                random_params.append(param2.detach().cpu().numpy().flatten())
        
        trained_weights = np.concatenate(trained_params)
        random_weights = np.concatenate(random_params)
        
        weight_diff = np.mean(np.abs(trained_weights - random_weights))
        
        if weight_diff > 0.01:
            print(f"‚úÖ ÊùÉÈáçÈ™åËØÅÈÄöËøá (Â∑ÆÂºÇ: {weight_diff:.6f})")
        else:
            raise ValueError(f"‚ö†Ô∏è Ê®°ÂûãÊùÉÈáçÂèØËÉΩÊú™ÂÖÖÂàÜËÆ≠ÁªÉ (Â∑ÆÂºÇ: {weight_diff:.6f})")
    
    def preprocess_video(self, video_data, is_base64=True):
        """È¢ÑÂ§ÑÁêÜËßÜÈ¢ëÊï∞ÊçÆ"""
        try:
            # Â§ÑÁêÜbase64ÁºñÁ†ÅÁöÑËßÜÈ¢ë
            if is_base64:
                video_bytes = base64.b64decode(video_data)
                
                # ‰øùÂ≠òÂà∞‰∏¥Êó∂Êñá‰ª∂
                with tempfile.NamedTemporaryFile(suffix='.mp4', delete=False) as temp_file:
                    temp_file.write(video_bytes)
                    temp_path = temp_file.name
            else:
                temp_path = video_data
            
            # ÊèêÂèñËßÜÈ¢ëÂ∏ß
            frames = self._extract_frames(temp_path)
            
            # Ê∏ÖÁêÜ‰∏¥Êó∂Êñá‰ª∂
            if is_base64 and os.path.exists(temp_path):
                os.unlink(temp_path)
            
            return frames
            
        except Exception as e:
            raise ValueError(f"ËßÜÈ¢ëÈ¢ÑÂ§ÑÁêÜÂ§±Ë¥•: {e}")
    
    def _extract_frames(self, video_path, num_frames=8):
        """‰ªéËßÜÈ¢ë‰∏≠ÊèêÂèñÂ∏ß"""
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError("Êó†Ê≥ïÊâìÂºÄËßÜÈ¢ëÊñá‰ª∂")
        
        frames = []
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        if total_frames > 0:
            # ÂùáÂåÄÈááÊ†∑Â∏ß
            frame_indices = np.linspace(0, total_frames-1, num_frames, dtype=int)
            
            for frame_idx in frame_indices:
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
                ret, frame = cap.read()
                if ret:
                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    frame = cv2.resize(frame, (128, 128))
                    frames.append(frame)
                else:
                    if frames:
                        frames.append(frames[-1])
                    else:
                        frames.append(np.zeros((128, 128, 3), dtype=np.uint8))
        
        cap.release()
        
        # Á°Æ‰øùÂ∏ßÊï∞
        while len(frames) < num_frames:
            if frames:
                frames.append(frames[-1])
            else:
                frames.append(np.zeros((128, 128, 3), dtype=np.uint8))
        
        # ËΩ¨Êç¢‰∏∫tensor
        frames = np.array(frames[:num_frames])
        frames = torch.from_numpy(frames).float() / 255.0
        frames = frames.permute(0, 3, 1, 2)  # [T, C, H, W]
        
        # Â∫îÁî®ÂèòÊç¢
        frames_list = []
        for i in range(frames.shape[0]):
            frame = self.transforms(frames[i])
            frames_list.append(frame)
        frames = torch.stack(frames_list).unsqueeze(0)  # [1, T, C, H, W]
        
        return frames.to(self.device)
    
    def predict_emotions(self, video_data, is_base64=True):
        """È¢ÑÊµãÊÉÖÊÑü"""
        try:
            # È¢ÑÂ§ÑÁêÜËßÜÈ¢ë
            frames = self.preprocess_video(video_data, is_base64)
            
            # Ê®°ÂûãÈ¢ÑÊµã
            with torch.no_grad():
                raw_output = self.model(frames).cpu().numpy()[0]
            
            # Â∞ÜÂéüÂßãËæìÂá∫ËΩ¨Êç¢‰∏∫0-3ËåÉÂõ¥ÔºàÂ¶ÇÊûúÈúÄË¶ÅÔºâ
            emotions_raw = np.clip(raw_output, 0, 3)
            
            # ËÆ°ÁÆóÊÉÖÊÑüÂç†ÊØîÔºàËΩØmaxÂΩí‰∏ÄÂåñÔºâ
            emotions_exp = np.exp(emotions_raw - np.max(emotions_raw))  # Êï∞ÂÄºÁ®≥ÂÆö
            emotions_prob = emotions_exp / np.sum(emotions_exp)
            
            # ËÆ°ÁÆóÂä†ÊùÉÊÉÖÊÑüÂàÜÊï∞
            weighted_score = 0
            for i, emotion in enumerate(self.emotion_names):
                weighted_score += emotions_prob[i] * self.emotion_weights[emotion]
            
            # Â∞ÜÂàÜÊï∞Êò†Â∞ÑÂà∞0-100ËåÉÂõ¥Ôºå50‰∏∫‰∏≠ÊÄß
            final_score = 50 + weighted_score * 25  # [-2, 2] -> [0, 100]
            final_score = np.clip(final_score, 0, 100)
            
            # ÊûÑÂª∫ÁªìÊûú
            result = {
                'timestamp': datetime.now().isoformat(),
                'emotions': {
                    'raw_scores': {
                        emotion: float(emotions_raw[i]) 
                        for i, emotion in enumerate(self.emotion_names)
                    },
                    'probabilities': {
                        emotion: float(emotions_prob[i]) 
                        for i, emotion in enumerate(self.emotion_names)
                    },
                    'percentages': {
                        emotion: float(emotions_prob[i] * 100) 
                        for i, emotion in enumerate(self.emotion_names)
                    }
                },
                'final_score': float(final_score),
                'dominant_emotion': self.emotion_names[np.argmax(emotions_prob)],
                'confidence': float(np.max(emotions_prob)),
                'interpretation': self._interpret_score(final_score),
                'model_info': {
                    'mae': 0.6295,  # ÂΩìÂâçÊúÄ‰Ω≥ÊÄßËÉΩ
                    'accuracy_improvement': '+10.19%'
                }
            }
            
            return result
            
        except Exception as e:
            raise RuntimeError(f"ÊÉÖÊÑüÈ¢ÑÊµãÂ§±Ë¥•: {e}")
    
    def _interpret_score(self, score):
        """Ëß£ÈáäÊÉÖÊÑüÂàÜÊï∞"""
        if score >= 75:
            return "ÁßØÊûÅÁä∂ÊÄÅ - È´òÂ∫¶ÂèÇ‰∏éÂíå‰∏ìÊ≥®"
        elif score >= 60:
            return "ËâØÂ•ΩÁä∂ÊÄÅ - ÁßØÊûÅÂèÇ‰∏é"
        elif score >= 40:
            return "‰∏≠ÊÄßÁä∂ÊÄÅ - Ê≠£Â∏∏Ë°®Áé∞"
        elif score >= 25:
            return "ËΩªÂæÆÊ∂àÊûÅ - ÂèØËÉΩÂá∫Áé∞Âõ∞ÊÉëÊàñÊó†ËÅä"
        else:
            return "Ê∂àÊûÅÁä∂ÊÄÅ - ÊòéÊòæÁöÑÊå´ÊäòÊàñ‰∏•ÈáçÊó†ËÅä"

# Flask API
app = Flask(__name__)

# ÂÖ®Â±ÄAPIÂÆû‰æã
api_instance = None

def initialize_api():
    """ÂàùÂßãÂåñAPI"""
    global api_instance
    if api_instance is None:
        try:
            api_instance = EmotionRecognitionAPI()
            print("üöÄ APIÊúçÂä°ÂêØÂä®ÊàêÂäü")
        except Exception as e:
            print(f"‚ùå APIÂàùÂßãÂåñÂ§±Ë¥•: {e}")
            raise

@app.route('/health', methods=['GET'])
def health_check():
    """ÂÅ•Â∫∑Ê£ÄÊü•"""
    if api_instance is None:
        initialize_api()
    return jsonify({
        'status': 'healthy',
        'timestamp': datetime.now().isoformat(),
        'model_loaded': api_instance is not None,
        'device': str(device)
    })

@app.route('/predict', methods=['POST'])
def predict():
    """ÊÉÖÊÑüÈ¢ÑÊµãÊé•Âè£"""
    if api_instance is None:
        initialize_api()
    if api_instance is None:
        return jsonify({'error': 'APIÊú™ÂàùÂßãÂåñ'}), 500
    
    try:
        data = request.get_json()
        
        if 'video' not in data:
            return jsonify({'error': 'Áº∫Â∞ëvideoÂ≠óÊÆµ'}), 400
        
        # È¢ÑÊµãÊÉÖÊÑü
        result = api_instance.predict_emotions(
            data['video'], 
            is_base64=data.get('is_base64', True)
        )
        
        return jsonify(result)
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/predict_file', methods=['POST'])
def predict_file():
    """Êñá‰ª∂‰∏ä‰º†È¢ÑÊµãÊé•Âè£"""
    if api_instance is None:
        initialize_api()
    if api_instance is None:
        return jsonify({'error': 'APIÊú™ÂàùÂßãÂåñ'}), 500
    
    try:
        if 'video' not in request.files:
            return jsonify({'error': 'Áº∫Â∞ëvideoÊñá‰ª∂'}), 400
        
        file = request.files['video']
        if file.filename == '':
            return jsonify({'error': 'Êú™ÈÄâÊã©Êñá‰ª∂'}), 400
        
        # ‰øùÂ≠òÂà∞‰∏¥Êó∂Êñá‰ª∂
        with tempfile.NamedTemporaryFile(suffix='.mp4', delete=False) as temp_file:
            file.save(temp_file.name)
            temp_path = temp_file.name
        
        # È¢ÑÊµãÊÉÖÊÑü
        result = api_instance.predict_emotions(temp_path, is_base64=False)
        
        # Ê∏ÖÁêÜ‰∏¥Êó∂Êñá‰ª∂
        os.unlink(temp_path)
        
        return jsonify(result)
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

def main():
    """‰∏ªÂáΩÊï∞"""
    print("üéØ DAiSEEÊÉÖÊÑüËØÜÂà´APIÊúçÂä°")
    print("=" * 50)
    
    # È™åËØÅÊ®°ÂûãÊñá‰ª∂
    if not os.path.exists('best_fast_daisee_model.pth'):
        print("‚ùå Ê®°ÂûãÊñá‰ª∂‰∏çÂ≠òÂú®: best_fast_daisee_model.pth")
        return
    
    # ÂêØÂä®APIÊúçÂä°
    app.run(host='0.0.0.0', port=5000, debug=False)

if __name__ == "__main__":
    main() 