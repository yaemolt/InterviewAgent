#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DAiSEEæƒ…æ„Ÿè¯†åˆ«APIæ¥å£
è¾“å‡ºå››ä¸ªæƒ…æ„Ÿæ ‡ç­¾ã€å æ¯”å’Œæœ€ç»ˆæ‰“åˆ†
"""

import os
import torch
import torch.nn as nn
import numpy as np
import cv2
from torchvision import transforms
from flask import Flask, request, jsonify
import base64
import tempfile
import json
from datetime import datetime

# è®¾ç½®è®¾å¤‡
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class FastEmotionModel(nn.Module):
    """å¿«é€Ÿè½»é‡åŒ–æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹ - æ­£ç¡®æ¶æ„"""
    
    def __init__(self, sequence_length=8, num_classes=4):
        super(FastEmotionModel, self).__init__()
        
        # è½»é‡åŒ–CNNç‰¹å¾æå–å™¨
        self.conv_layers = nn.Sequential(
            # Block 1 - å¿«é€Ÿä¸‹é‡‡æ ·
            nn.Conv2d(3, 32, kernel_size=5, stride=2, padding=2),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),  # 128 -> 32
            
            # Block 2 
            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),  # 32 -> 16
            
            # Block 3 - æœ€ç»ˆç‰¹å¾
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool2d((4, 4))  # 16 -> 4
        )
        
        # ç®€åŒ–çš„LSTM
        self.lstm = nn.LSTM(
            input_size=128 * 16,
            hidden_size=64,  # æ›´å°çš„éšè—å±‚
            num_layers=1,
            batch_first=True
        )
        
        # æç®€åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(64, 32),
            nn.ReLU(inplace=True),
            nn.Linear(32, num_classes)
        )
        
        self._initialize_weights()
    
    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        batch_size, seq_len, c, h, w = x.size()
        
        # CNNç‰¹å¾æå–
        x = x.view(batch_size * seq_len, c, h, w)
        features = self.conv_layers(x)  # (batch*seq, 128, 4, 4)
        features = features.view(features.size(0), -1)  # (batch*seq, 2048)
        
        # é‡å¡‘ä¸ºåºåˆ—
        features = features.view(batch_size, seq_len, -1)  # (batch, seq, 2048)
        
        # LSTM - åªå–æœ€åä¸€ä¸ªè¾“å‡º
        lstm_out, (hidden, _) = self.lstm(features)
        last_output = hidden[-1]  # å–æœ€åä¸€å±‚çš„éšè—çŠ¶æ€ (batch, 64)
        
        # åˆ†ç±»
        output = self.classifier(last_output)
        
        return output

class EmotionRecognitionAPI:
    """æƒ…æ„Ÿè¯†åˆ«APIç±»"""
    
    def __init__(self, model_path='best_fast_daisee_model.pth'):
        self.device = device
        self.emotion_names = ['Boredom', 'Engagement', 'Confusion', 'Frustration']
        self.emotion_weights = {
            'Boredom': -1.0,      # æ— èŠæ˜¯è´Ÿé¢æƒ…ç»ª
            'Engagement': 1.0,    # å‚ä¸æ˜¯æ­£é¢æƒ…ç»ª
            'Confusion': -0.5,    # å›°æƒ‘æ˜¯è½»å¾®è´Ÿé¢
            'Frustration': -1.5   # æŒ«æŠ˜æ˜¯å¼ºçƒˆè´Ÿé¢
        }
        
        # åŠ è½½æ¨¡å‹
        self.model = self._load_model(model_path)
        
        # æ•°æ®é¢„å¤„ç†
        self.transforms = transforms.Compose([
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        print(f"ğŸ¯ æƒ…æ„Ÿè¯†åˆ«APIåˆå§‹åŒ–å®Œæˆ")
        print(f"   è®¾å¤‡: {self.device}")
        print(f"   æ¨¡å‹è·¯å¾„: {model_path}")
    
    def _load_model(self, model_path):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        checkpoint = torch.load(model_path, map_location=self.device)
        
        # éªŒè¯æ¨¡å‹æ˜¯å¦å·²è®­ç»ƒ
        if 'val_mae' not in checkpoint:
            raise ValueError("æ¨¡å‹æ–‡ä»¶æ— æ•ˆï¼šç¼ºå°‘éªŒè¯ä¿¡æ¯")
        
        print(f"ğŸ“Š æ¨¡å‹ä¿¡æ¯:")
        print(f"   è®­ç»ƒè½®æ•°: {checkpoint.get('epoch', 'N/A')}")
        print(f"   éªŒè¯MAE: {checkpoint.get('val_mae', 'N/A'):.4f}")
        
        # åˆ›å»ºæ¨¡å‹å®ä¾‹
        model = FastEmotionModel().to(self.device)
        
        # åŠ è½½æƒé‡
        try:
            model.load_state_dict(checkpoint['model_state_dict'])
            print(f"âœ… æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ")
        except Exception as e:
            raise RuntimeError(f"æ¨¡å‹æƒé‡åŠ è½½å¤±è´¥: {e}")
        
        model.eval()
        
        # éªŒè¯æ¨¡å‹æƒé‡ééšæœº
        self._verify_model_weights(model, checkpoint)
        
        return model
    
    def _verify_model_weights(self, model, checkpoint):
        """éªŒè¯æ¨¡å‹æƒé‡æ˜¯å¦å·²è®­ç»ƒï¼ˆééšæœºï¼‰"""
        # åˆ›å»ºéšæœºæ¨¡å‹å¯¹æ¯”
        random_model = FastEmotionModel().to(self.device)
        
        # æ¯”è¾ƒæƒé‡å·®å¼‚
        trained_params = []
        random_params = []
        
        for (name1, param1), (name2, param2) in zip(model.named_parameters(), random_model.named_parameters()):
            if param1.requires_grad:
                trained_params.append(param1.detach().cpu().numpy().flatten())
                random_params.append(param2.detach().cpu().numpy().flatten())
        
        trained_weights = np.concatenate(trained_params)
        random_weights = np.concatenate(random_params)
        
        weight_diff = np.mean(np.abs(trained_weights - random_weights))
        
        if weight_diff > 0.01:
            print(f"âœ… æƒé‡éªŒè¯é€šè¿‡ (å·®å¼‚: {weight_diff:.6f})")
        else:
            raise ValueError(f"âš ï¸ æ¨¡å‹æƒé‡å¯èƒ½æœªå……åˆ†è®­ç»ƒ (å·®å¼‚: {weight_diff:.6f})")
    
    def preprocess_video(self, video_data, is_base64=True):
        """é¢„å¤„ç†è§†é¢‘æ•°æ®"""
        try:
            # å¤„ç†base64ç¼–ç çš„è§†é¢‘
            if is_base64:
                video_bytes = base64.b64decode(video_data)
                
                # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
                with tempfile.NamedTemporaryFile(suffix='.mp4', delete=False) as temp_file:
                    temp_file.write(video_bytes)
                    temp_path = temp_file.name
            else:
                temp_path = video_data
            
            # æå–è§†é¢‘å¸§
            frames = self._extract_frames(temp_path)
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if is_base64 and os.path.exists(temp_path):
                os.unlink(temp_path)
            
            return frames
            
        except Exception as e:
            raise ValueError(f"è§†é¢‘é¢„å¤„ç†å¤±è´¥: {e}")
    
    def _extract_frames(self, video_path, num_frames=8):
        """ä»è§†é¢‘ä¸­æå–å¸§"""
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError("æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶")
        
        frames = []
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        if total_frames > 0:
            # å‡åŒ€é‡‡æ ·å¸§
            frame_indices = np.linspace(0, total_frames-1, num_frames, dtype=int)
            
            for frame_idx in frame_indices:
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
                ret, frame = cap.read()
                if ret:
                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    frame = cv2.resize(frame, (128, 128))
                    frames.append(frame)
                else:
                    if frames:
                        frames.append(frames[-1])
                    else:
                        frames.append(np.zeros((128, 128, 3), dtype=np.uint8))
        
        cap.release()
        
        # ç¡®ä¿å¸§æ•°
        while len(frames) < num_frames:
            if frames:
                frames.append(frames[-1])
            else:
                frames.append(np.zeros((128, 128, 3), dtype=np.uint8))
        
        # è½¬æ¢ä¸ºtensor
        frames = np.array(frames[:num_frames])
        frames = torch.from_numpy(frames).float() / 255.0
        frames = frames.permute(0, 3, 1, 2)  # [T, C, H, W]
        
        # åº”ç”¨å˜æ¢
        frames_list = []
        for i in range(frames.shape[0]):
            frame = self.transforms(frames[i])
            frames_list.append(frame)
        frames = torch.stack(frames_list).unsqueeze(0)  # [1, T, C, H, W]
        
        return frames.to(self.device)
    
    def predict_emotions(self, video_data, is_base64=True):
        """é¢„æµ‹æƒ…æ„Ÿ"""
        try:
            # é¢„å¤„ç†è§†é¢‘
            frames = self.preprocess_video(video_data, is_base64)
            
            # æ¨¡å‹é¢„æµ‹
            with torch.no_grad():
                raw_output = self.model(frames).cpu().numpy()[0]
            
            # å°†åŸå§‹è¾“å‡ºè½¬æ¢ä¸º0-3èŒƒå›´ï¼ˆå¦‚æœéœ€è¦ï¼‰
            emotions_raw = np.clip(raw_output, 0, 3)
            
            # è®¡ç®—æƒ…æ„Ÿå æ¯”ï¼ˆè½¯maxå½’ä¸€åŒ–ï¼‰
            emotions_exp = np.exp(emotions_raw - np.max(emotions_raw))  # æ•°å€¼ç¨³å®š
            emotions_prob = emotions_exp / np.sum(emotions_exp)
            
            # è®¡ç®—åŠ æƒæƒ…æ„Ÿåˆ†æ•°
            weighted_score = 0
            for i, emotion in enumerate(self.emotion_names):
                weighted_score += emotions_prob[i] * self.emotion_weights[emotion]
            
            # å°†åˆ†æ•°æ˜ å°„åˆ°0-100èŒƒå›´ï¼Œ50ä¸ºä¸­æ€§
            final_score = 50 + weighted_score * 25  # [-2, 2] -> [0, 100]
            final_score = np.clip(final_score, 0, 100)
            
            # æ„å»ºç»“æœ
            result = {
                'timestamp': datetime.now().isoformat(),
                'emotions': {
                    'raw_scores': {
                        emotion: float(emotions_raw[i]) 
                        for i, emotion in enumerate(self.emotion_names)
                    },
                    'probabilities': {
                        emotion: float(emotions_prob[i]) 
                        for i, emotion in enumerate(self.emotion_names)
                    },
                    'percentages': {
                        emotion: float(emotions_prob[i] * 100) 
                        for i, emotion in enumerate(self.emotion_names)
                    }
                },
                'final_score': float(final_score),
                'dominant_emotion': self.emotion_names[np.argmax(emotions_prob)],
                'confidence': float(np.max(emotions_prob)),
                'interpretation': self._interpret_score(final_score),
                'model_info': {
                    'mae': 0.6295,  # å½“å‰æœ€ä½³æ€§èƒ½
                    'accuracy_improvement': '+10.19%'
                }
            }
            
            return result
            
        except Exception as e:
            raise RuntimeError(f"æƒ…æ„Ÿé¢„æµ‹å¤±è´¥: {e}")
    
    def _interpret_score(self, score):
        """è§£é‡Šæƒ…æ„Ÿåˆ†æ•°"""
        if score >= 75:
            return "ç§¯æçŠ¶æ€ - é«˜åº¦å‚ä¸å’Œä¸“æ³¨"
        elif score >= 60:
            return "è‰¯å¥½çŠ¶æ€ - ç§¯æå‚ä¸"
        elif score >= 40:
            return "ä¸­æ€§çŠ¶æ€ - æ­£å¸¸è¡¨ç°"
        elif score >= 25:
            return "è½»å¾®æ¶ˆæ - å¯èƒ½å‡ºç°å›°æƒ‘æˆ–æ— èŠ"
        else:
            return "æ¶ˆæçŠ¶æ€ - æ˜æ˜¾çš„æŒ«æŠ˜æˆ–ä¸¥é‡æ— èŠ"

# Flask API
app = Flask(__name__)

# å…¨å±€APIå®ä¾‹
api_instance = None

def initialize_api():
    """åˆå§‹åŒ–API"""
    global api_instance
    if api_instance is None:
        try:
            api_instance = EmotionRecognitionAPI()
            print("ğŸš€ APIæœåŠ¡å¯åŠ¨æˆåŠŸ")
        except Exception as e:
            print(f"âŒ APIåˆå§‹åŒ–å¤±è´¥: {e}")
            raise

@app.route('/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥"""
    if api_instance is None:
        initialize_api()
    return jsonify({
        'status': 'healthy',
        'timestamp': datetime.now().isoformat(),
        'model_loaded': api_instance is not None,
        'device': str(device)
    })

@app.route('/predict', methods=['POST'])
def predict():
    """æƒ…æ„Ÿé¢„æµ‹æ¥å£"""
    if api_instance is None:
        initialize_api()
    if api_instance is None:
        return jsonify({'error': 'APIæœªåˆå§‹åŒ–'}), 500
    
    try:
        data = request.get_json()
        
        if 'video' not in data:
            return jsonify({'error': 'ç¼ºå°‘videoå­—æ®µ'}), 400
        
        # é¢„æµ‹æƒ…æ„Ÿ
        result = api_instance.predict_emotions(
            data['video'], 
            is_base64=data.get('is_base64', True)
        )
        
        return jsonify(result)
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/predict_file', methods=['POST'])
def predict_file():
    """æ–‡ä»¶ä¸Šä¼ é¢„æµ‹æ¥å£"""
    if api_instance is None:
        initialize_api()
    if api_instance is None:
        return jsonify({'error': 'APIæœªåˆå§‹åŒ–'}), 500
    
    try:
        if 'video' not in request.files:
            return jsonify({'error': 'ç¼ºå°‘videoæ–‡ä»¶'}), 400
        
        file = request.files['video']
        if file.filename == '':
            return jsonify({'error': 'æœªé€‰æ‹©æ–‡ä»¶'}), 400
        
        # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(suffix='.mp4', delete=False) as temp_file:
            file.save(temp_file.name)
            temp_path = temp_file.name
        
        # é¢„æµ‹æƒ…æ„Ÿ
        result = api_instance.predict_emotions(temp_path, is_base64=False)
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.unlink(temp_path)
        
        return jsonify(result)
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ DAiSEEæƒ…æ„Ÿè¯†åˆ«APIæœåŠ¡")
    print("=" * 50)
    
    # éªŒè¯æ¨¡å‹æ–‡ä»¶
    if not os.path.exists('best_fast_daisee_model.pth'):
        print("âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: best_fast_daisee_model.pth")
        return
    
    # å¯åŠ¨APIæœåŠ¡
    app.run(host='0.0.0.0', port=5000, debug=False)

if __name__ == "__main__":
    main() 