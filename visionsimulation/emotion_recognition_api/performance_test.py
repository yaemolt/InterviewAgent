#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
DAiSEEæ¨¡å‹æ€§èƒ½æµ‹è¯• - 2ç§’è§†é¢‘å¤„ç†æ—¶é—´æµ‹è¯•
"""

import torch
import torch.nn as nn
import time
import numpy as np

print("="*70)
print("â±ï¸  DAiSEEæ¨¡å‹æ€§èƒ½æµ‹è¯• - 2ç§’è§†é¢‘å¤„ç†æ—¶é—´")
print("="*70)

# ç®€åŒ–çš„CNN3Dæ¨¡å‹ï¼ˆé¿å…torchvisionä¾èµ–ï¼‰
class SimpleCNN3D(nn.Module):
    def __init__(self, num_emotions=4):
        super().__init__()
        
        self.conv3d_layers = nn.Sequential(
            # ç¬¬ä¸€å±‚3Då·ç§¯
            nn.Conv3d(3, 64, kernel_size=(3, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3)),
            nn.BatchNorm3d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1)),
            
            # ç¬¬äºŒå±‚3Då·ç§¯
            nn.Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1)),
            nn.BatchNorm3d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)),
            
            # ç¬¬ä¸‰å±‚3Då·ç§¯
            nn.Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1)),
            nn.BatchNorm3d(256),
            nn.ReLU(inplace=True),
            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)),
            
            # å…¨å±€å¹³å‡æ± åŒ–
            nn.AdaptiveAvgPool3d((1, 1, 1))
        )
        
        # æƒ…æ„Ÿåˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Dropout(0.5),
            nn.Linear(256, 128),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(128, num_emotions),
            nn.Sigmoid()  # å°†è¾“å‡ºæ˜ å°„åˆ°0-1ï¼Œç„¶åä¹˜ä»¥3å¾—åˆ°0-3èŒƒå›´
        )
        
    def forward(self, x):
        # x: (batch_size, sequence_length, channels, height, width)
        # è½¬æ¢ä¸º3Då·ç§¯æ ¼å¼: (batch_size, channels, sequence_length, height, width)
        x = x.permute(0, 2, 1, 3, 4)
        
        features = self.conv3d_layers(x)
        output = self.classifier(features)
        
        # æ˜ å°„åˆ°0-3èŒƒå›´
        return output * 3.0

# ç®€åŒ–çš„2D CNN + LSTMæ¨¡å‹
class SimpleResNetLSTM(nn.Module):
    def __init__(self, num_emotions=4, hidden_dim=256):
        super().__init__()
        
        # ç®€åŒ–çš„2D CNNç‰¹å¾æå–å™¨
        self.feature_extractor = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),
            
            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            
            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten()
        )
        
        # LSTMæ—¶åºå»ºæ¨¡
        self.lstm = nn.LSTM(
            input_size=256,
            hidden_size=hidden_dim,
            num_layers=2,
            batch_first=True,
            dropout=0.3
        )
        
        # åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim, 128),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(128, num_emotions),
            nn.Sigmoid()
        )
        
    def forward(self, x):
        batch_size, seq_len, c, h, w = x.size()
        
        # é€å¸§æå–ç‰¹å¾
        frame_features = []
        for i in range(seq_len):
            frame = x[:, i, :, :, :]  # (batch_size, c, h, w)
            features = self.feature_extractor(frame)  # (batch_size, 256)
            frame_features.append(features)
        
        # å †å ä¸ºåºåˆ—
        sequence_features = torch.stack(frame_features, dim=1)  # (batch_size, seq_len, 256)
        
        # LSTMå¤„ç†
        lstm_out, _ = self.lstm(sequence_features)  # (batch_size, seq_len, hidden_dim)
        
        # ä½¿ç”¨æœ€åä¸€å¸§çš„è¾“å‡º
        final_output = lstm_out[:, -1, :]  # (batch_size, hidden_dim)
        
        # åˆ†ç±»
        emotion_scores = self.classifier(final_output) * 3.0
        
        return emotion_scores

def benchmark_model(model, input_tensor, model_name, num_runs=10):
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    model.eval()
    
    # é¢„çƒ­
    with torch.no_grad():
        for _ in range(3):
            _ = model(input_tensor)
    
    # è®¡æ—¶æµ‹è¯•
    times = []
    with torch.no_grad():
        for i in range(num_runs):
            start_time = time.time()
            output = model(input_tensor)
            end_time = time.time()
            times.append(end_time - start_time)
    
    avg_time = np.mean(times)
    std_time = np.std(times)
    min_time = np.min(times)
    max_time = np.max(times)
    
    print(f"\nğŸ“Š {model_name} æ€§èƒ½æµ‹è¯•ç»“æœ:")
    print(f"   å¹³å‡æ—¶é—´: {avg_time*1000:.1f} Â± {std_time*1000:.1f} ms")
    print(f"   æœ€å¿«æ—¶é—´: {min_time*1000:.1f} ms")
    print(f"   æœ€æ…¢æ—¶é—´: {max_time*1000:.1f} ms")
    print(f"   è¾“å‡ºç¤ºä¾‹: {output[0].numpy()}")
    
    return avg_time

def main():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nğŸ’» æµ‹è¯•è®¾å¤‡: {device}")
    print(f"   PyTorchç‰ˆæœ¬: {torch.__version__}")
    
    # 2ç§’è§†é¢‘å‚æ•°
    fps = 30  # æ¯ç§’30å¸§
    video_duration = 2  # 2ç§’
    total_frames = fps * video_duration  # 60å¸§
    sequence_length = 16  # æ¨¡å‹è¾“å…¥åºåˆ—é•¿åº¦
    height, width = 224, 224  # å›¾åƒåˆ†è¾¨ç‡
    
    print(f"\nğŸ“¹ 2ç§’è§†é¢‘å‚æ•°:")
    print(f"   è§†é¢‘æ—¶é•¿: {video_duration}ç§’")
    print(f"   å¸§ç‡: {fps} fps")
    print(f"   æ€»å¸§æ•°: {total_frames}å¸§")
    print(f"   é‡‡æ ·å¸§æ•°: {sequence_length}å¸§")
    print(f"   å›¾åƒåˆ†è¾¨ç‡: {height}x{width}")
    
    # åˆ›å»ºæ¨¡æ‹Ÿ2ç§’è§†é¢‘æ•°æ®ï¼ˆæ‰¹é‡å¤§å°ä¸º1ï¼‰
    batch_size = 1
    video_tensor = torch.randn(batch_size, sequence_length, 3, height, width).to(device)
    
    print(f"   è¾“å…¥å¼ é‡å½¢çŠ¶: {video_tensor.shape}")
    print(f"   æ•°æ®å¤§å°: {video_tensor.numel() * 4 / 1024 / 1024:.2f} MB")
    
    # æµ‹è¯•CNN3Dæ¨¡å‹
    print(f"\nğŸ§  æµ‹è¯•æ¨¡å‹ 1: ç®€åŒ–CNN3D")
    cnn3d_model = SimpleCNN3D(num_emotions=4).to(device)
    params_3d = sum(p.numel() for p in cnn3d_model.parameters())
    print(f"   å‚æ•°é‡: {params_3d:,} ({params_3d/1e6:.1f}M)")
    
    time_3d = benchmark_model(cnn3d_model, video_tensor, "CNN3D")
    
    # æµ‹è¯•ResNet+LSTMæ¨¡å‹
    print(f"\nğŸ§  æµ‹è¯•æ¨¡å‹ 2: ç®€åŒ–ResNet+LSTM")
    lstm_model = SimpleResNetLSTM(num_emotions=4, hidden_dim=256).to(device)
    params_lstm = sum(p.numel() for p in lstm_model.parameters())
    print(f"   å‚æ•°é‡: {params_lstm:,} ({params_lstm/1e6:.1f}M)")
    
    time_lstm = benchmark_model(lstm_model, video_tensor, "ResNet+LSTM")
    
    # åˆ†æä¸åŒåˆ†è¾¨ç‡çš„å½±å“
    print(f"\nğŸ“ åˆ†è¾¨ç‡å½±å“æµ‹è¯•:")
    resolutions = [(112, 112), (224, 224), (448, 448)]
    
    for h, w in resolutions:
        test_tensor = torch.randn(1, sequence_length, 3, h, w).to(device)
        data_size = test_tensor.numel() * 4 / 1024 / 1024
        
        start_time = time.time()
        with torch.no_grad():
            _ = cnn3d_model(test_tensor)
        end_time = time.time()
        
        print(f"   {h}x{w}: {(end_time-start_time)*1000:.1f} ms (æ•°æ®é‡: {data_size:.1f} MB)")
    
    # æ€»ç»“
    print(f"\n" + "="*70)
    print(f"ğŸ“‹ 2ç§’è§†é¢‘å¤„ç†æ—¶é—´æ€»ç»“")
    print(f"="*70)
    print(f"ğŸ”¸ CNN3Dæ¨¡å‹:")
    print(f"   å¤„ç†æ—¶é—´: {time_3d*1000:.1f} ms")
    print(f"   å¤„ç†é€Ÿåº¦: {video_duration/time_3d:.1f}x å®æ—¶é€Ÿåº¦")
    print(f"   å‚æ•°é‡: {params_3d/1e6:.1f}M")
    
    print(f"\nğŸ”¸ ResNet+LSTMæ¨¡å‹:")
    print(f"   å¤„ç†æ—¶é—´: {time_lstm*1000:.1f} ms")
    print(f"   å¤„ç†é€Ÿåº¦: {video_duration/time_lstm:.1f}x å®æ—¶é€Ÿåº¦")
    print(f"   å‚æ•°é‡: {params_lstm/1e6:.1f}M")
    
    print(f"\nğŸ’¡ æ€§èƒ½åˆ†æ:")
    if time_3d < 0.1:  # å°äº100ms
        print(f"   âœ… CNN3Då¯ä»¥å®æ—¶å¤„ç† (< 100ms)")
    else:
        print(f"   âš ï¸ CNN3Då¤„ç†è¾ƒæ…¢ (> 100ms)")
    
    if time_lstm < 0.2:  # å°äº200ms
        print(f"   âœ… ResNet+LSTMå¯ä»¥å‡†å®æ—¶å¤„ç† (< 200ms)")
    else:
        print(f"   âš ï¸ ResNet+LSTMå¤„ç†è¾ƒæ…¢ (> 200ms)")
    
    print(f"\nğŸš€ ä¼˜åŒ–å»ºè®®:")
    print(f"   - é™ä½è¾“å…¥åˆ†è¾¨ç‡ (224â†’112) å¯æé€Ÿçº¦ 4x")
    print(f"   - å‡å°‘åºåˆ—é•¿åº¦ (16â†’8) å¯æé€Ÿçº¦ 2x") 
    print(f"   - ä½¿ç”¨GPUå¯æé€Ÿ 10-50x")
    print(f"   - æ¨¡å‹é‡åŒ–å¯æé€Ÿ 2-4x")
    
    fps_3d = 1.0 / time_3d
    fps_lstm = 1.0 / time_lstm
    print(f"\nğŸ“ˆ ç†è®ºå¤„ç†èƒ½åŠ›:")
    print(f"   CNN3D: {fps_3d:.1f} ä¸ª2ç§’è§†é¢‘/ç§’")
    print(f"   ResNet+LSTM: {fps_lstm:.1f} ä¸ª2ç§’è§†é¢‘/ç§’")

if __name__ == "__main__":
    main() 