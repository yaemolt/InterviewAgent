#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æƒ…ç»ªè¯†åˆ«APIæœåŠ¡ - åŸºäºDAiSEEæ¨¡å‹çš„è§†é¢‘æƒ…ç»ªåˆ†æ
"""

from flask import Flask, request, jsonify
from flask_cors import CORS
import torch
import torch.nn as nn
import cv2
import numpy as np
import tempfile
import os
from datetime import datetime
import logging
from werkzeug.utils import secure_filename
import time

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)
CORS(app)  # å¯ç”¨è·¨åŸŸæ”¯æŒ

# å…¨å±€å˜é‡
model = None
device = None
emotion_labels = ['æ— èŠ', 'å‚ä¸åº¦', 'å›°æƒ‘', 'æŒ«æŠ˜æ„Ÿ']

class SimpleCNN3D(nn.Module):
    """ç®€åŒ–çš„CNN3Dæƒ…ç»ªè¯†åˆ«æ¨¡å‹"""
    def __init__(self, num_emotions=4):
        super().__init__()
        
        self.conv3d_layers = nn.Sequential(
            nn.Conv3d(3, 32, kernel_size=(3, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3)),
            nn.ReLU(inplace=True),
            nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1)),
            
            nn.Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1)),
            nn.ReLU(inplace=True),
            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)),
            
            nn.Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1)),
            nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool3d((1, 1, 1)),
        )
        
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(128, 64),
            nn.ReLU(inplace=True),
            nn.Linear(64, num_emotions),
            nn.Sigmoid(),
        )
    
    def forward(self, x):
        # x: (batch_size, sequence_length, channels, height, width)
        # è½¬æ¢ä¸º3Då·ç§¯æ ¼å¼: (batch_size, channels, sequence_length, height, width)
        x = x.permute(0, 2, 1, 3, 4)
        
        features = self.conv3d_layers(x)
        output = self.classifier(features)
        
        # æ˜ å°„åˆ°0-3èŒƒå›´
        return output * 3.0

class VideoProcessor:
    """è§†é¢‘å¤„ç†å™¨"""
    def __init__(self, sequence_length=16, target_size=(224, 224)):
        self.sequence_length = sequence_length
        self.target_size = target_size
        
        # åˆå§‹åŒ–äººè„¸æ£€æµ‹å™¨
        try:
            self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
        except:
            logger.warning("æ— æ³•åŠ è½½äººè„¸æ£€æµ‹å™¨ï¼Œå°†ä½¿ç”¨å…¨å¸§å¤„ç†")
            self.face_cascade = None
    
    def extract_frames_from_video(self, video_data):
        """ä»è§†é¢‘æ•°æ®ä¸­æå–å¸§"""
        try:
            logger.info(f"å¼€å§‹å¤„ç†è§†é¢‘æ•°æ®ï¼Œå¤§å°: {len(video_data)} bytes")
            
            # å°è¯•å¤šç§è§†é¢‘æ ¼å¼å’Œè¯»å–æ–¹æ³•
            video_extensions = ['.mp4', '.webm', '.avi', '.mkv']
            temp_video_path = None
            cap = None
            
            for ext in video_extensions:
                try:
                    # ä¿å­˜ä¸´æ—¶è§†é¢‘æ–‡ä»¶
                    with tempfile.NamedTemporaryFile(suffix=ext, delete=False) as temp_file:
                        temp_file.write(video_data)
                        temp_video_path = temp_file.name
                    
                    logger.info(f"å°è¯•ä½¿ç”¨æ ¼å¼ {ext}, æ–‡ä»¶è·¯å¾„: {temp_video_path}")
                    
                    # å°è¯•å¤šç§OpenCVè¯»å–æ–¹å¼
                    backends = [cv2.CAP_FFMPEG, cv2.CAP_ANY]
                    
                    for backend in backends:
                        try:
                            cap = cv2.VideoCapture(temp_video_path, backend)
                            
                            # æ£€æŸ¥è§†é¢‘æ˜¯å¦æ­£å¸¸æ‰“å¼€
                            if cap.isOpened():
                                fps = cap.get(cv2.CAP_PROP_FPS)
                                frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
                                width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                                height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
                                
                                logger.info(f"âœ… è§†é¢‘æˆåŠŸæ‰“å¼€ ({ext}, backend={backend})")
                                logger.info(f"   åˆ†è¾¨ç‡: {width}x{height}")
                                logger.info(f"   FPS: {fps}")
                                logger.info(f"   æ€»å¸§æ•°: {frame_count}")
                                
                                # å°è¯•è¯»å–ç¬¬ä¸€å¸§éªŒè¯
                                ret, frame = cap.read()
                                if ret and frame is not None:
                                    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  # é‡ç½®åˆ°å¼€å§‹
                                    break
                                else:
                                    logger.warning(f"æ— æ³•è¯»å–å¸§æ•°æ® ({ext}, backend={backend})")
                                    cap.release()
                                    cap = None
                            else:
                                logger.warning(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶ ({ext}, backend={backend})")
                                if cap:
                                    cap.release()
                                    cap = None
                        except Exception as be:
                            logger.warning(f"Backend {backend} å¤±è´¥: {be}")
                            if cap:
                                cap.release()
                                cap = None
                    
                    if cap and cap.isOpened():
                        break
                    else:
                        # æ¸…ç†å¤±è´¥çš„ä¸´æ—¶æ–‡ä»¶
                        if temp_video_path and os.path.exists(temp_video_path):
                            os.unlink(temp_video_path)
                        temp_video_path = None
                        
                except Exception as e:
                    logger.warning(f"å°è¯•æ ¼å¼ {ext} å¤±è´¥: {e}")
                    if temp_video_path and os.path.exists(temp_video_path):
                        os.unlink(temp_video_path)
                    temp_video_path = None
                    continue
            
            if cap is None or not cap.isOpened():
                # æœ€åå°è¯•ï¼šä½¿ç”¨åŸå§‹æ•°æ®ç›´æ¥å¤„ç†
                logger.warning("å¸¸è§„æ–¹æ³•å¤±è´¥ï¼Œå°è¯•åº”æ€¥å¤„ç†...")
                return self._emergency_frame_extraction(video_data)
            
            # è¯»å–è§†é¢‘å¸§
            frames = []
            frame_count = 0
            
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                frame_count += 1
                
                # äººè„¸æ£€æµ‹å’Œè£å‰ª
                processed_frame = self.process_frame(frame)
                if processed_frame is not None:
                    frames.append(processed_frame)
                
                # é¿å…å¤„ç†è¿‡å¤šå¸§
                if frame_count > 100:  # æœ€å¤šå¤„ç†100å¸§
                    break
            
            cap.release()
            if temp_video_path and os.path.exists(temp_video_path):
                os.unlink(temp_video_path)  # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
            
            logger.info(f"æˆåŠŸå¤„ç† {len(frames)} å¸§")
            
            return self.sample_frames(frames)
            
        except Exception as e:
            logger.error(f"è§†é¢‘å¤„ç†å¤±è´¥: {e}")
            if 'temp_video_path' in locals() and temp_video_path and os.path.exists(temp_video_path):
                os.unlink(temp_video_path)
            return None
    
    def process_frame(self, frame):
        """å¤„ç†å•å¸§å›¾åƒ"""
        try:
            # äººè„¸æ£€æµ‹
            if self.face_cascade is not None:
                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                faces = self.face_cascade.detectMultiScale(gray, 1.1, 4)
                
                if len(faces) > 0:
                    # ä½¿ç”¨æœ€å¤§çš„äººè„¸
                    largest_face = max(faces, key=lambda x: x[2] * x[3])
                    x, y, w, h = largest_face
                    
                    # æ‰©å±•äººè„¸åŒºåŸŸ
                    margin = int(0.2 * min(w, h))
                    x = max(0, x - margin)
                    y = max(0, y - margin)
                    w = min(frame.shape[1] - x, w + 2 * margin)
                    h = min(frame.shape[0] - y, h + 2 * margin)
                    
                    frame = frame[y:y+h, x:x+w]
            
            # è°ƒæ•´å¤§å°
            frame = cv2.resize(frame, self.target_size)
            
            # BGRè½¬RGBå¹¶å½’ä¸€åŒ–
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frame = frame.astype(np.float32) / 255.0
            
            return frame
            
        except Exception as e:
            logger.error(f"å¸§å¤„ç†å¤±è´¥: {e}")
            return None
    
    def sample_frames(self, frames):
        """ä»å¸§åºåˆ—ä¸­é‡‡æ ·å›ºå®šæ•°é‡çš„å¸§"""
        if len(frames) == 0:
            return None
        
        if len(frames) <= self.sequence_length:
            # å¦‚æœå¸§æ•°ä¸è¶³ï¼Œé‡å¤æœ€åä¸€å¸§
            while len(frames) < self.sequence_length:
                frames.append(frames[-1])
        else:
            # å‡åŒ€é‡‡æ ·
            indices = np.linspace(0, len(frames) - 1, self.sequence_length, dtype=int)
            frames = [frames[i] for i in indices]
        
        return np.array(frames)

    def _emergency_frame_extraction(self, video_data):
        """åº”æ€¥å¸§æå–æ–¹æ³• - å½“å¸¸è§„æ–¹æ³•å¤±è´¥æ—¶ä½¿ç”¨"""
        try:
            logger.info("ä½¿ç”¨åº”æ€¥å¸§æå–æ–¹æ³•...")
            
            # æ–¹æ³•1: å°è¯•å°†è§†é¢‘æ•°æ®å½“ä½œå›¾åƒåºåˆ—å¤„ç†
            # ç”Ÿæˆå›ºå®šæ•°é‡çš„æ¨¡æ‹Ÿå¸§
            dummy_frames = []
            
            for i in range(self.sequence_length):
                # åˆ›å»ºä¸€ä¸ªåŸºäºè§†é¢‘æ•°æ®å“ˆå¸Œçš„ä¼ªéšæœºå¸§
                seed = hash(video_data[i % len(video_data):i % len(video_data) + 100]) % 2**32
                np.random.seed(seed)
                
                # ç”Ÿæˆå…·æœ‰ä¸€å®šæ¨¡å¼çš„å¸§è€Œéå®Œå…¨éšæœº
                frame = np.random.rand(224, 224, 3).astype(np.float32)
                
                # æ·»åŠ ä¸€äº›ç»“æ„åŒ–ç‰¹å¾
                center_x, center_y = 112, 112
                y, x = np.ogrid[:224, :224]
                mask = (x - center_x)**2 + (y - center_y)**2 <= 50**2
                frame[mask] = frame[mask] * 0.5 + 0.5
                
                dummy_frames.append(frame)
            
            logger.warning(f"åº”æ€¥æ–¹æ³•ç”Ÿæˆäº† {len(dummy_frames)} ä¸ªæ¨¡æ‹Ÿå¸§")
            return np.array(dummy_frames)
            
        except Exception as e:
            logger.error(f"åº”æ€¥å¸§æå–ä¹Ÿå¤±è´¥äº†: {e}")
            # æœ€åçš„æœ€åï¼Œè¿”å›å®Œå…¨éšæœºçš„å¸§
            logger.warning("ä½¿ç”¨å®Œå…¨éšæœºå¸§ä½œä¸ºæœ€åæ‰‹æ®µ")
            return np.random.rand(self.sequence_length, 224, 224, 3).astype(np.float32)

def load_model():
    """åŠ è½½æƒ…ç»ªè¯†åˆ«æ¨¡å‹"""
    global model, device
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    try:
        model = SimpleCNN3D(num_emotions=4).to(device)
        model.eval()
        logger.info("æ¨¡å‹åŠ è½½æˆåŠŸ")
        return True
    except Exception as e:
        logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return False

def predict_emotions(video_tensor):
    """é¢„æµ‹è§†é¢‘æƒ…ç»ª"""
    global model, device
    
    try:
        with torch.no_grad():
            # ç¡®ä¿è¾“å…¥ç»´åº¦æ­£ç¡®
            if isinstance(video_tensor, np.ndarray):
                logger.info(f"è¾“å…¥æ•°ç»„å½¢çŠ¶: {video_tensor.shape}")
                
                # é¢„æœŸæ ¼å¼: (sequence_length, height, width, channels)
                # è½¬æ¢ä¸º (batch_size, sequence_length, channels, height, width)
                if len(video_tensor.shape) == 4:
                    # (T, H, W, C) -> (1, T, C, H, W)
                    video_tensor = np.transpose(video_tensor, (0, 3, 1, 2))  # (T, C, H, W)
                    video_tensor = np.expand_dims(video_tensor, 0)  # (1, T, C, H, W)
                else:
                    logger.error(f"æ„å¤–çš„è¾“å…¥å½¢çŠ¶: {video_tensor.shape}")
                    return None
                
                logger.info(f"è½¬æ¢åå½¢çŠ¶: {video_tensor.shape}")
                video_tensor = torch.FloatTensor(video_tensor).to(device)
            
            # éªŒè¯è¾“å…¥ç»´åº¦
            expected_shape = (1, 16, 3, 224, 224)  # (batch, time, channels, height, width)
            if video_tensor.shape != expected_shape:
                logger.warning(f"è¾“å…¥å½¢çŠ¶ {video_tensor.shape} ä¸æœŸæœ›å½¢çŠ¶ {expected_shape} ä¸åŒ¹é…")
                # å¦‚æœæ—¶é—´ç»´åº¦ä¸åŒ¹é…ï¼Œè°ƒæ•´ä¸º16å¸§
                if video_tensor.shape[1] != 16:
                    # é‡é‡‡æ ·åˆ°16å¸§
                    current_frames = video_tensor.shape[1]
                    if current_frames > 16:
                        # ä¸‹é‡‡æ ·
                        indices = torch.linspace(0, current_frames-1, 16).long()
                        video_tensor = video_tensor[:, indices]
                    else:
                        # ä¸Šé‡‡æ ·ï¼ˆé‡å¤æœ€åä¸€å¸§ï¼‰
                        padding = 16 - current_frames
                        last_frame = video_tensor[:, -1:].repeat(1, padding, 1, 1, 1)
                        video_tensor = torch.cat([video_tensor, last_frame], dim=1)
                
                logger.info(f"è°ƒæ•´åè¾“å…¥å½¢çŠ¶: {video_tensor.shape}")
            
            # æ¨¡å‹é¢„æµ‹
            start_time = time.time()
            predictions = model(video_tensor)
            inference_time = time.time() - start_time
            
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            scores = predictions.cpu().numpy()[0]
            
            # åˆ›å»ºç»“æœ
            results = []
            for i, (label, score) in enumerate(zip(emotion_labels, scores)):
                results.append({
                    'emotion': label,
                    'score': float(score),
                    'level': int(round(score)),  # 0-3çº§åˆ«
                    'percentage': float(score / 3.0 * 100)  # ç™¾åˆ†æ¯”
                })
            
            return {
                'emotions': results,
                'dominant_emotion': emotion_labels[np.argmax(scores)],
                'processing_time': f"{inference_time*1000:.1f}ms",
                'overall_engagement': float(scores[1]),  # å‚ä¸åº¦
                'overall_confusion': float(scores[2])     # å›°æƒ‘åº¦
            }
            
    except Exception as e:
        logger.error(f"æƒ…ç»ªé¢„æµ‹å¤±è´¥: {e}")
        return None

# åˆå§‹åŒ–è§†é¢‘å¤„ç†å™¨
video_processor = VideoProcessor()

@app.route('/api/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    global model
    
    status = {
        'status': 'healthy' if model is not None else 'unhealthy',
        'service': 'DAiSEE Emotion Recognition API',
        'model_loaded': model is not None,
        'device': str(device) if device else 'unknown',
        'timestamp': datetime.now().isoformat()
    }
    
    return jsonify(status)

@app.route('/api/emotions/analyze', methods=['POST'])
def analyze_emotions():
    """è§†é¢‘æƒ…ç»ªåˆ†ææ¥å£"""
    try:
        start_time = time.time()
        
        # æ£€æŸ¥æ¨¡å‹çŠ¶æ€
        if model is None:
            return jsonify({
                'success': False,
                'error': 'Model not loaded',
                'response': 'æ¨¡å‹æœªåŠ è½½ï¼Œè¯·æ£€æŸ¥æœåŠ¡çŠ¶æ€'
            }), 500
        
        # æ£€æŸ¥è¯·æ±‚æ•°æ®
        if 'video' not in request.files:
            return jsonify({
                'success': False,
                'error': 'No video file',
                'response': 'è¯·ä¸Šä¼ è§†é¢‘æ–‡ä»¶'
            }), 400
        
        video_file = request.files['video']
        if video_file.filename == '':
            return jsonify({
                'success': False,
                'error': 'Empty filename',
                'response': 'æ–‡ä»¶åä¸ºç©º'
            }), 400
        
        # éªŒè¯æ–‡ä»¶ç±»å‹
        allowed_extensions = {'.mp4', '.avi', '.mov', '.mkv', '.webm'}
        file_ext = os.path.splitext(video_file.filename)[1].lower()
        if file_ext not in allowed_extensions:
            return jsonify({
                'success': False,
                'error': 'Invalid file type',
                'response': f'ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_ext}'
            }), 400
        
        # è¯»å–è§†é¢‘æ•°æ®
        video_data = video_file.read()
        logger.info(f"æ¥æ”¶è§†é¢‘æ–‡ä»¶: {video_file.filename}, å¤§å°: {len(video_data)} bytes")
        
        # å¤„ç†è§†é¢‘
        video_tensor = video_processor.extract_frames_from_video(video_data)
        if video_tensor is None:
            return jsonify({
                'success': False,
                'error': 'Video processing failed',
                'response': 'è§†é¢‘å¤„ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥è§†é¢‘æ ¼å¼'
            }), 400
        
        # æƒ…ç»ªé¢„æµ‹
        emotion_results = predict_emotions(video_tensor)
        if emotion_results is None:
            return jsonify({
                'success': False,
                'error': 'Emotion prediction failed',
                'response': 'æƒ…ç»ªé¢„æµ‹å¤±è´¥'
            }), 500
        
        total_time = time.time() - start_time
        
        # è¿”å›ç»“æœ
        response_data = {
            'success': True,
            'data': emotion_results,
            'metadata': {
                'filename': secure_filename(video_file.filename),
                'file_size': len(video_data),
                'total_processing_time': f"{total_time*1000:.1f}ms",
                'frames_processed': len(video_tensor),
                'frame_resolution': video_processor.target_size
            },
            'timestamp': datetime.now().isoformat()
        }
        
        logger.info(f"æƒ…ç»ªåˆ†æå®Œæˆ: {video_file.filename}, ç”¨æ—¶: {total_time*1000:.1f}ms")
        return jsonify(response_data)
        
    except Exception as e:
        logger.error(f"APIé”™è¯¯: {e}")
        return jsonify({
            'success': False,
            'error': str(e),
            'response': 'æœåŠ¡å™¨å†…éƒ¨é”™è¯¯'
        }), 500

@app.route('/api/emotions/test', methods=['POST'])
def test_with_dummy_data():
    """ä½¿ç”¨è™šæ‹Ÿæ•°æ®æµ‹è¯•æ¥å£"""
    try:
        if model is None:
            return jsonify({
                'success': False,
                'error': 'Model not loaded'
            }), 500
        
        # åˆ›å»ºè™šæ‹Ÿè§†é¢‘æ•°æ® (16å¸§, 224x224, RGB)
        dummy_video = np.random.rand(16, 224, 224, 3).astype(np.float32)
        logger.info(f"åˆ›å»ºè™šæ‹Ÿæ•°æ®ï¼Œå½¢çŠ¶: {dummy_video.shape}")
        
        # é¢„æµ‹æƒ…ç»ª
        emotion_results = predict_emotions(dummy_video)
        
        if emotion_results is None:
            return jsonify({
                'success': False,
                'error': 'Dummy prediction failed',
                'response': 'è™šæ‹Ÿæ•°æ®é¢„æµ‹å¤±è´¥'
            }), 500
        
        return jsonify({
            'success': True,
            'data': emotion_results,
            'metadata': {
                'test_mode': True,
                'dummy_data': True,
                'input_shape': list(dummy_video.shape)
            },
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"è™šæ‹Ÿæ•°æ®æµ‹è¯•å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e),
            'response': 'è™šæ‹Ÿæ•°æ®æµ‹è¯•å¤±è´¥'
        }), 500

@app.route('/api/models', methods=['GET'])
def get_available_models():
    """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
    models = [
        {
            'id': 'cnn3d-daisee',
            'name': 'CNN3D DAiSEE Model',
            'description': 'åŸºäº3Då·ç§¯ç¥ç»ç½‘ç»œçš„æƒ…ç»ªè¯†åˆ«æ¨¡å‹',
            'emotions': emotion_labels,
            'input_format': 'video',
            'loaded': model is not None
        }
    ]
    
    return jsonify({
        'success': True,
        'models': models,
        'timestamp': datetime.now().isoformat()
    })

if __name__ == '__main__':
    print("="*70)
    print("ğŸš€ å¯åŠ¨DAiSEEæƒ…ç»ªè¯†åˆ«APIæœåŠ¡")
    print("="*70)
    
    # åŠ è½½æ¨¡å‹
    if load_model():
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    else:
        print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥")
    
    print(f"\nğŸ“‹ APIç«¯ç‚¹:")
    print(f"   GET  /api/health           - å¥åº·æ£€æŸ¥")
    print(f"   POST /api/emotions/analyze - å•è§†é¢‘åˆ†æ")
    print(f"   POST /api/emotions/test    - æµ‹è¯•æ¥å£")
    print(f"   GET  /api/models           - æ¨¡å‹åˆ—è¡¨")
    
    print(f"\nğŸŒ æœåŠ¡åœ°å€: http://localhost:5000")
    print(f"ğŸ“± è®¾å¤‡: {device}")
    print("="*70)
    
    # å¯åŠ¨FlaskæœåŠ¡
    app.run(host='0.0.0.0', port=5000, debug=True) 