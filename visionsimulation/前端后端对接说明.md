# æ™ºèƒ½æƒ…ç»ªè¯†åˆ«ç³»ç»Ÿ - å‰ç«¯åç«¯å¯¹æ¥è¯´æ˜

## ğŸ“‹ é¡¹ç›®æ¦‚è¿°

æœ¬ç³»ç»ŸåŸºäºDAiSEEæ•°æ®é›†è®­ç»ƒçš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå®ç°å®æ—¶è§†é¢‘æƒ…ç»ªè¯†åˆ«ã€‚å‰ç«¯ä½¿ç”¨Vue 3ï¼Œåç«¯ä½¿ç”¨Python Flask + PyTorchã€‚

### ğŸ¯ æ”¯æŒçš„æƒ…ç»ªç»´åº¦
1. **æ— èŠ (Boredom)** - 0-3åˆ†
2. **å‚ä¸åº¦ (Engagement)** - 0-3åˆ†  
3. **å›°æƒ‘ (Confusion)** - 0-3åˆ†
4. **æŒ«æŠ˜æ„Ÿ (Frustration)** - 0-3åˆ†

---

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

```
ğŸ“± å‰ç«¯ (Vue 3 + Vite)
    â†“ HTTPè¯·æ±‚
ğŸŒ APIä»£ç† (/api -> localhost:5000)
    â†“
ğŸ§  åç«¯ (Flask + PyTorch)
    â†“
ğŸ¯ DAiSEEæƒ…ç»ªè¯†åˆ«æ¨¡å‹
```

---

## ğŸ”Œ APIæ¥å£å¯¹æ¥

### 1. å¥åº·æ£€æŸ¥æ¥å£

**å‰ç«¯è°ƒç”¨:**
```javascript
import { checkAPIHealth } from '@/api/emotion.js'

const response = await checkAPIHealth()
// GET /api/health
```

**åç«¯å“åº”:**
```json
{
  "status": "healthy",
  "service": "DAiSEE Emotion Recognition API",
  "model_loaded": true,
  "device": "cpu",
  "timestamp": "2024-01-01T00:00:00.000Z"
}
```

### 2. è§†é¢‘æƒ…ç»ªåˆ†ææ¥å£

**å‰ç«¯è°ƒç”¨:**
```javascript
import { recognizeEmotion } from '@/api/emotion.js'

// å‘é€è§†é¢‘Blobï¼ˆ8å¸§ï¼Œ1ç§’@8fpsï¼‰
const response = await recognizeEmotion(videoBlob)
// POST /api/emotions/analyze
```

**åç«¯å“åº”:**
```json
{
  "success": true,
  "data": {
    "emotions": [
      {"emotion": "æ— èŠ", "score": 1.25, "level": 1, "percentage": 41.7},
      {"emotion": "å‚ä¸åº¦", "score": 2.35, "level": 2, "percentage": 78.3},
      {"emotion": "å›°æƒ‘", "score": 0.85, "level": 1, "percentage": 28.3},
      {"emotion": "æŒ«æŠ˜æ„Ÿ", "score": 1.15, "level": 1, "percentage": 38.3}
    ],
    "dominant_emotion": "å‚ä¸åº¦",
    "processing_time": "137.5ms",
    "overall_engagement": 2.35,
    "overall_confusion": 0.85
  },
  "metadata": {
    "filename": "emotion_clip.webm",
    "file_size": 2048576,
    "total_processing_time": "245.8ms",
    "frames_processed": 16,
    "frame_resolution": [224, 224]
  },
  "timestamp": "2024-01-01T00:00:00.000Z"
}
```

---

## ğŸ¥ è§†é¢‘å¤„ç†æµç¨‹

### å‰ç«¯æµç¨‹
1. **æ‘„åƒå¤´åˆå§‹åŒ–**: `getUserMedia({ frameRate: 8 })`
2. **è§†é¢‘å½•åˆ¶**: `MediaRecorder.start(1000)` æ¯ç§’è§¦å‘
3. **æ•°æ®å‘é€**: æ¯æ¬¡`ondataavailable`æ”¶åˆ°8å¸§è§†é¢‘
4. **ç»“æœå¤„ç†**: è§£æåç«¯4ç»´æƒ…ç»ªæ•°æ®

### åç«¯æµç¨‹
1. **æ¥æ”¶è§†é¢‘**: Flaskæ¥æ”¶multipart/form-data
2. **å¸§æå–**: OpenCVæå–è§†é¢‘å¸§
3. **é¢„å¤„ç†**: è°ƒæ•´å¤§å°åˆ°224x224ï¼Œå½’ä¸€åŒ–
4. **æ¨¡å‹æ¨ç†**: 3D CNNå¤„ç†16å¸§åºåˆ—
5. **ç»“æœè¿”å›**: 4ç»´æƒ…ç»ªè¯„åˆ†

---

## ğŸ› ï¸ å‰ç«¯å…³é”®ä»£ç 

### APIé…ç½® (`src/api/emotion.js`)
```javascript
import axios from 'axios'

const api = axios.create({
  baseURL: '/api',
  timeout: 30000
})

export const recognizeEmotion = async (videoBlob) => {
  const formData = new FormData()
  formData.append('video', videoBlob, 'emotion_clip.webm')
  const response = await api.post('/emotions/analyze', formData)
  return response
}

export const checkAPIHealth = async () => {
  const response = await api.get('/health')
  return response
}
```

### æ‘„åƒå¤´å¤„ç† (`CameraEmotionPage.vue`)
```javascript
// æ‘„åƒå¤´çº¦æŸ - ç¡®ä¿8fps
const constraints = {
  video: {
    frameRate: { ideal: 8, max: 8 },
    width: { ideal: 640, max: 1280 },
    height: { ideal: 480, max: 720 },
    facingMode: 'user'
  }
}

// è§†é¢‘å½•åˆ¶ - æ¯ç§’æ”¶é›†8å¸§
mediaRecorder.start(1000)
mediaRecorder.ondataavailable = async (event) => {
  if (event.data && event.data.size > 0) {
    await handleVideoData(event.data) // å‘é€åˆ°åç«¯
  }
}
```

### æ•°æ®å¤„ç†
```javascript
const handleVideoData = async (videoBlob) => {
  const response = await recognizeEmotion(videoBlob)
  if (response.data && response.data.success) {
    const data = response.data.data
    const emotionData = {
      emotion: data.dominant_emotion,
      confidence: Math.round(data.overall_engagement * 100 / 3),
      overallEngagement: data.overall_engagement,
      overallConfusion: data.overall_confusion,
      allEmotions: data.emotions,
      processingTime: data.processing_time
    }
    emotionResults.value.push(emotionData)
  }
}
```

---

## ğŸ”§ åç«¯å…³é”®ä»£ç 

### APIè·¯ç”± (`emotion_api.py`)
```python
@app.route('/api/emotions/analyze', methods=['POST'])
def analyze_emotions():
    # æ¥æ”¶è§†é¢‘æ–‡ä»¶
    video_file = request.files['video']
    video_data = video_file.read()
    
    # å¤„ç†è§†é¢‘
    video_tensor = video_processor.extract_frames_from_video(video_data)
    
    # æƒ…ç»ªé¢„æµ‹
    emotion_results = predict_emotions(video_tensor)
    
    return jsonify({
        'success': True,
        'data': emotion_results,
        'metadata': {...}
    })
```

### æ¨¡å‹é¢„æµ‹
```python
def predict_emotions(video_tensor):
    with torch.no_grad():
        predictions = model(video_tensor)
        scores = predictions.cpu().numpy()[0]
        
        results = []
        for i, (label, score) in enumerate(zip(emotion_labels, scores)):
            results.append({
                'emotion': label,
                'score': float(score),
                'level': int(round(score)),
                'percentage': float(score / 3.0 * 100)
            })
        
        return {
            'emotions': results,
            'dominant_emotion': emotion_labels[np.argmax(scores)],
            'overall_engagement': float(scores[1]),
            'overall_confusion': float(scores[2])
        }
```

---

## ğŸš€ å¯åŠ¨æ–¹å¼

### æ–¹å¼1: ä¸€é”®å¯åŠ¨ï¼ˆæ¨èï¼‰
```bash
# åŒå‡»è¿è¡Œ
start-emotion-system.bat
```

### æ–¹å¼2: åˆ†åˆ«å¯åŠ¨

**å¯åŠ¨åç«¯:**
```bash
cd emotion_recognition_api
python start_api.py
```

**å¯åŠ¨å‰ç«¯:**
```bash
cd Frontend
npm install
npm run dev
```

---

## ğŸ“Š æ•°æ®æµè½¬ç¤ºä¾‹

### 1. åˆå§‹åŒ–æ£€æŸ¥
```
å‰ç«¯ â†’ GET /api/health â†’ åç«¯
å‰ç«¯ â† {"status": "healthy", "model_loaded": true} â† åç«¯
```

### 2. è§†é¢‘åˆ†æå¾ªç¯
```
å‰ç«¯æ¯ç§’: MediaRecorder â†’ 8å¸§è§†é¢‘Blob
å‰ç«¯ â†’ POST /api/emotions/analyze + FormData(video) â†’ åç«¯
å‰ç«¯ â† {"success": true, "data": {...4ç»´æƒ…ç»ªæ•°æ®...}} â† åç«¯
å‰ç«¯ç•Œé¢æ›´æ–°: æ˜¾ç¤ºæœ€æ–°æƒ…ç»ª + ç´¯ç§¯å†å²
```

### 3. ç»“æœå±•ç¤º
```
ç”¨æˆ·ç‚¹å‡»"ç»“æŸè¯†åˆ«" â†’ å‰ç«¯åˆ‡æ¢åˆ°EmotionResultç»„ä»¶
å±•ç¤º: ä¸»å¯¼æƒ…ç»ªã€å¹³å‡å‚ä¸åº¦ã€æƒ…ç»ªåˆ†å¸ƒå›¾ã€è¯¦ç»†è®°å½•
æ”¯æŒ: JSONæ•°æ®å¯¼å‡º
```

---

## âš ï¸ æ³¨æ„äº‹é¡¹

### æŠ€æœ¯è¦æ±‚
- **å‰ç«¯**: Node.js 16+, ç°ä»£æµè§ˆå™¨(Chrome 88+)
- **åç«¯**: Python 3.8-3.11, PyTorch, OpenCV
- **ç½‘ç»œ**: localhost:5175 â†” localhost:5000

### æ€§èƒ½å‚è€ƒ
- **è§†é¢‘å¤„ç†**: ~137ms/ç§’(8å¸§)
- **æ¨¡å‹æ¨ç†**: ~45ms
- **å†…å­˜ä½¿ç”¨**: ~2GB (åŠ è½½æ¨¡å‹å)

### å¸¸è§é—®é¢˜
1. **æ‘„åƒå¤´æƒé™**: ç¡®ä¿ä½¿ç”¨HTTPSæˆ–localhost
2. **APIè¿æ¥å¤±è´¥**: æ£€æŸ¥åç«¯æ˜¯å¦åœ¨5000ç«¯å£è¿è¡Œ
3. **æ¨¡å‹åŠ è½½æ…¢**: é¦–æ¬¡å¯åŠ¨éœ€è¦æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…
4. **è§†é¢‘æ ¼å¼**: å‰ç«¯ç”Ÿæˆwebmï¼Œåç«¯æ”¯æŒå¤šç§æ ¼å¼

---

## ğŸ”„ æ‰©å±•æ€§è¯´æ˜

### æ·»åŠ æ–°æƒ…ç»ªç»´åº¦
1. åç«¯: ä¿®æ”¹`emotion_labels`æ•°ç»„
2. å‰ç«¯: æ›´æ–°æƒ…ç»ªæ˜¾ç¤ºé€»è¾‘

### æ›´æ¢æ¨¡å‹
1. åç«¯: æ›¿æ¢æ¨¡å‹æ–‡ä»¶ï¼Œæ›´æ–°`load_model()`
2. å‰ç«¯: æ— éœ€ä¿®æ”¹ï¼ˆæ¥å£å…¼å®¹ï¼‰

### æ€§èƒ½ä¼˜åŒ–
1. åç«¯: ä½¿ç”¨GPUåŠ é€Ÿï¼Œæ‰¹å¤„ç†
2. å‰ç«¯: å‡å°‘è§†é¢‘åˆ†è¾¨ç‡ï¼Œé™ä½å¸§ç‡

---

**ç‰ˆæœ¬**: v1.0  
**åˆ›å»ºæ—¶é—´**: 2024å¹´  
**ç»´æŠ¤è€…**: æ™ºèƒ½æƒ…ç»ªè¯†åˆ«ç³»ç»Ÿå¼€å‘å›¢é˜Ÿ 