# 智能情绪识别系统 - 前端后端对接说明

## 📋 项目概述

本系统基于DAiSEE数据集训练的深度学习模型，实现实时视频情绪识别。前端使用Vue 3，后端使用Python Flask + PyTorch。

### 🎯 支持的情绪维度
1. **无聊 (Boredom)** - 0-3分
2. **参与度 (Engagement)** - 0-3分  
3. **困惑 (Confusion)** - 0-3分
4. **挫折感 (Frustration)** - 0-3分

---

## 🏗️ 系统架构

```
📱 前端 (Vue 3 + Vite)
    ↓ HTTP请求
🌐 API代理 (/api -> localhost:5000)
    ↓
🧠 后端 (Flask + PyTorch)
    ↓
🎯 DAiSEE情绪识别模型
```

---

## 🔌 API接口对接

### 1. 健康检查接口

**前端调用:**
```javascript
import { checkAPIHealth } from '@/api/emotion.js'

const response = await checkAPIHealth()
// GET /api/health
```

**后端响应:**
```json
{
  "status": "healthy",
  "service": "DAiSEE Emotion Recognition API",
  "model_loaded": true,
  "device": "cpu",
  "timestamp": "2024-01-01T00:00:00.000Z"
}
```

### 2. 视频情绪分析接口

**前端调用:**
```javascript
import { recognizeEmotion } from '@/api/emotion.js'

// 发送视频Blob（8帧，1秒@8fps）
const response = await recognizeEmotion(videoBlob)
// POST /api/emotions/analyze
```

**后端响应:**
```json
{
  "success": true,
  "data": {
    "emotions": [
      {"emotion": "无聊", "score": 1.25, "level": 1, "percentage": 41.7},
      {"emotion": "参与度", "score": 2.35, "level": 2, "percentage": 78.3},
      {"emotion": "困惑", "score": 0.85, "level": 1, "percentage": 28.3},
      {"emotion": "挫折感", "score": 1.15, "level": 1, "percentage": 38.3}
    ],
    "dominant_emotion": "参与度",
    "processing_time": "137.5ms",
    "overall_engagement": 2.35,
    "overall_confusion": 0.85
  },
  "metadata": {
    "filename": "emotion_clip.webm",
    "file_size": 2048576,
    "total_processing_time": "245.8ms",
    "frames_processed": 16,
    "frame_resolution": [224, 224]
  },
  "timestamp": "2024-01-01T00:00:00.000Z"
}
```

---

## 🎥 视频处理流程

### 前端流程
1. **摄像头初始化**: `getUserMedia({ frameRate: 8 })`
2. **视频录制**: `MediaRecorder.start(1000)` 每秒触发
3. **数据发送**: 每次`ondataavailable`收到8帧视频
4. **结果处理**: 解析后端4维情绪数据

### 后端流程
1. **接收视频**: Flask接收multipart/form-data
2. **帧提取**: OpenCV提取视频帧
3. **预处理**: 调整大小到224x224，归一化
4. **模型推理**: 3D CNN处理16帧序列
5. **结果返回**: 4维情绪评分

---

## 🛠️ 前端关键代码

### API配置 (`src/api/emotion.js`)
```javascript
import axios from 'axios'

const api = axios.create({
  baseURL: '/api',
  timeout: 30000
})

export const recognizeEmotion = async (videoBlob) => {
  const formData = new FormData()
  formData.append('video', videoBlob, 'emotion_clip.webm')
  const response = await api.post('/emotions/analyze', formData)
  return response
}

export const checkAPIHealth = async () => {
  const response = await api.get('/health')
  return response
}
```

### 摄像头处理 (`CameraEmotionPage.vue`)
```javascript
// 摄像头约束 - 确保8fps
const constraints = {
  video: {
    frameRate: { ideal: 8, max: 8 },
    width: { ideal: 640, max: 1280 },
    height: { ideal: 480, max: 720 },
    facingMode: 'user'
  }
}

// 视频录制 - 每秒收集8帧
mediaRecorder.start(1000)
mediaRecorder.ondataavailable = async (event) => {
  if (event.data && event.data.size > 0) {
    await handleVideoData(event.data) // 发送到后端
  }
}
```

### 数据处理
```javascript
const handleVideoData = async (videoBlob) => {
  const response = await recognizeEmotion(videoBlob)
  if (response.data && response.data.success) {
    const data = response.data.data
    const emotionData = {
      emotion: data.dominant_emotion,
      confidence: Math.round(data.overall_engagement * 100 / 3),
      overallEngagement: data.overall_engagement,
      overallConfusion: data.overall_confusion,
      allEmotions: data.emotions,
      processingTime: data.processing_time
    }
    emotionResults.value.push(emotionData)
  }
}
```

---

## 🔧 后端关键代码

### API路由 (`emotion_api.py`)
```python
@app.route('/api/emotions/analyze', methods=['POST'])
def analyze_emotions():
    # 接收视频文件
    video_file = request.files['video']
    video_data = video_file.read()
    
    # 处理视频
    video_tensor = video_processor.extract_frames_from_video(video_data)
    
    # 情绪预测
    emotion_results = predict_emotions(video_tensor)
    
    return jsonify({
        'success': True,
        'data': emotion_results,
        'metadata': {...}
    })
```

### 模型预测
```python
def predict_emotions(video_tensor):
    with torch.no_grad():
        predictions = model(video_tensor)
        scores = predictions.cpu().numpy()[0]
        
        results = []
        for i, (label, score) in enumerate(zip(emotion_labels, scores)):
            results.append({
                'emotion': label,
                'score': float(score),
                'level': int(round(score)),
                'percentage': float(score / 3.0 * 100)
            })
        
        return {
            'emotions': results,
            'dominant_emotion': emotion_labels[np.argmax(scores)],
            'overall_engagement': float(scores[1]),
            'overall_confusion': float(scores[2])
        }
```

---

## 🚀 启动方式

### 方式1: 一键启动（推荐）
```bash
# 双击运行
start-emotion-system.bat
```

### 方式2: 分别启动

**启动后端:**
```bash
cd emotion_recognition_api
python start_api.py
```

**启动前端:**
```bash
cd Frontend
npm install
npm run dev
```

---

## 📊 数据流转示例

### 1. 初始化检查
```
前端 → GET /api/health → 后端
前端 ← {"status": "healthy", "model_loaded": true} ← 后端
```

### 2. 视频分析循环
```
前端每秒: MediaRecorder → 8帧视频Blob
前端 → POST /api/emotions/analyze + FormData(video) → 后端
前端 ← {"success": true, "data": {...4维情绪数据...}} ← 后端
前端界面更新: 显示最新情绪 + 累积历史
```

### 3. 结果展示
```
用户点击"结束识别" → 前端切换到EmotionResult组件
展示: 主导情绪、平均参与度、情绪分布图、详细记录
支持: JSON数据导出
```

---

## ⚠️ 注意事项

### 技术要求
- **前端**: Node.js 16+, 现代浏览器(Chrome 88+)
- **后端**: Python 3.8-3.11, PyTorch, OpenCV
- **网络**: localhost:5175 ↔ localhost:5000

### 性能参考
- **视频处理**: ~137ms/秒(8帧)
- **模型推理**: ~45ms
- **内存使用**: ~2GB (加载模型后)

### 常见问题
1. **摄像头权限**: 确保使用HTTPS或localhost
2. **API连接失败**: 检查后端是否在5000端口运行
3. **模型加载慢**: 首次启动需要时间，请耐心等待
4. **视频格式**: 前端生成webm，后端支持多种格式

---

## 🔄 扩展性说明

### 添加新情绪维度
1. 后端: 修改`emotion_labels`数组
2. 前端: 更新情绪显示逻辑

### 更换模型
1. 后端: 替换模型文件，更新`load_model()`
2. 前端: 无需修改（接口兼容）

### 性能优化
1. 后端: 使用GPU加速，批处理
2. 前端: 减少视频分辨率，降低帧率

---

**版本**: v1.0  
**创建时间**: 2024年  
**维护者**: 智能情绪识别系统开发团队 