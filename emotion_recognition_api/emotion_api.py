#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æƒ…ç»ªè¯†åˆ«APIæœåŠ¡ - åŸºäºDAiSEEæ¨¡å‹çš„è§†é¢‘æƒ…ç»ªåˆ†æ
"""

from flask import Flask, request, jsonify
from flask_cors import CORS
import torch
import torch.nn as nn
import cv2
import numpy as np
import tempfile
import os
from datetime import datetime
import logging
from werkzeug.utils import secure_filename
import time

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)
CORS(app)  # å¯ç”¨è·¨åŸŸæ”¯æŒ

# å…¨å±€å˜é‡
model = None
device = None
emotion_labels = ['æ— èŠ', 'å‚ä¸åº¦', 'å›°æƒ‘', 'æŒ«æŠ˜æ„Ÿ']

class SimpleCNN3D(nn.Module):
    """ç®€åŒ–çš„CNN3Dæƒ…ç»ªè¯†åˆ«æ¨¡å‹"""
    def __init__(self, num_emotions=4):
        super().__init__()
        
        self.conv3d_layers = nn.Sequential(
            nn.Conv3d(3, 32, kernel_size=(3, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3)),
            nn.ReLU(inplace=True),
            nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1)),
            
            nn.Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1)),
            nn.ReLU(inplace=True),
            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)),
            
            nn.Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1)),
            nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool3d((1, 1, 1)),
        )
        
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(128, 64),
            nn.ReLU(inplace=True),
            nn.Linear(64, num_emotions),
            nn.Sigmoid(),
        )
    
    def forward(self, x):
        # x: (batch_size, sequence_length, channels, height, width)
        # è½¬æ¢ä¸º3Då·ç§¯æ ¼å¼: (batch_size, channels, sequence_length, height, width)
        x = x.permute(0, 2, 1, 3, 4)
        
        features = self.conv3d_layers(x)
        output = self.classifier(features)
        
        # æ˜ å°„åˆ°0-3èŒƒå›´
        return output * 3.0

class VideoProcessor:
    """è§†é¢‘å¤„ç†å™¨"""
    def __init__(self, sequence_length=16, target_size=(224, 224)):
        self.sequence_length = sequence_length
        self.target_size = target_size
        
        # åˆå§‹åŒ–äººè„¸æ£€æµ‹å™¨
        try:
            self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
        except:
            logger.warning("æ— æ³•åŠ è½½äººè„¸æ£€æµ‹å™¨ï¼Œå°†ä½¿ç”¨å…¨å¸§å¤„ç†")
            self.face_cascade = None
    
    def extract_frames_from_video(self, video_data):
        """ä»è§†é¢‘æ•°æ®ä¸­æå–å¸§"""
        try:
            # ä¿å­˜ä¸´æ—¶è§†é¢‘æ–‡ä»¶
            with tempfile.NamedTemporaryFile(suffix='.mp4', delete=False) as temp_file:
                temp_file.write(video_data)
                temp_video_path = temp_file.name
            
            # è¯»å–è§†é¢‘
            cap = cv2.VideoCapture(temp_video_path)
            frames = []
            
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                # äººè„¸æ£€æµ‹å’Œè£å‰ª
                processed_frame = self.process_frame(frame)
                if processed_frame is not None:
                    frames.append(processed_frame)
            
            cap.release()
            os.unlink(temp_video_path)  # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
            
            return self.sample_frames(frames)
            
        except Exception as e:
            logger.error(f"è§†é¢‘å¤„ç†å¤±è´¥: {e}")
            return None
    
    def process_frame(self, frame):
        """å¤„ç†å•å¸§å›¾åƒ"""
        try:
            # äººè„¸æ£€æµ‹
            if self.face_cascade is not None:
                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                faces = self.face_cascade.detectMultiScale(gray, 1.1, 4)
                
                if len(faces) > 0:
                    # ä½¿ç”¨æœ€å¤§çš„äººè„¸
                    largest_face = max(faces, key=lambda x: x[2] * x[3])
                    x, y, w, h = largest_face
                    
                    # æ‰©å±•äººè„¸åŒºåŸŸ
                    margin = int(0.2 * min(w, h))
                    x = max(0, x - margin)
                    y = max(0, y - margin)
                    w = min(frame.shape[1] - x, w + 2 * margin)
                    h = min(frame.shape[0] - y, h + 2 * margin)
                    
                    frame = frame[y:y+h, x:x+w]
            
            # è°ƒæ•´å¤§å°
            frame = cv2.resize(frame, self.target_size)
            
            # BGRè½¬RGBå¹¶å½’ä¸€åŒ–
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frame = frame.astype(np.float32) / 255.0
            
            return frame
            
        except Exception as e:
            logger.error(f"å¸§å¤„ç†å¤±è´¥: {e}")
            return None
    
    def sample_frames(self, frames):
        """ä»å¸§åºåˆ—ä¸­é‡‡æ ·å›ºå®šæ•°é‡çš„å¸§"""
        if len(frames) == 0:
            return None
        
        if len(frames) <= self.sequence_length:
            # å¦‚æœå¸§æ•°ä¸è¶³ï¼Œé‡å¤æœ€åä¸€å¸§
            while len(frames) < self.sequence_length:
                frames.append(frames[-1])
        else:
            # å‡åŒ€é‡‡æ ·
            indices = np.linspace(0, len(frames) - 1, self.sequence_length, dtype=int)
            frames = [frames[i] for i in indices]
        
        return np.array(frames)

def load_model():
    """åŠ è½½æƒ…ç»ªè¯†åˆ«æ¨¡å‹"""
    global model, device
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    try:
        model = SimpleCNN3D(num_emotions=4).to(device)
        model.eval()
        logger.info("æ¨¡å‹åŠ è½½æˆåŠŸ")
        return True
    except Exception as e:
        logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return False

def predict_emotions(video_tensor):
    """é¢„æµ‹è§†é¢‘æƒ…ç»ª"""
    global model, device
    
    try:
        with torch.no_grad():
            # è½¬æ¢ä¸ºå¼ é‡å¹¶ç§»åˆ°è®¾å¤‡
            if isinstance(video_tensor, np.ndarray):
                video_tensor = torch.FloatTensor(video_tensor).unsqueeze(0).to(device)
            
            # æ¨¡å‹é¢„æµ‹
            start_time = time.time()
            predictions = model(video_tensor)
            inference_time = time.time() - start_time
            
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            scores = predictions.cpu().numpy()[0]
            
            # åˆ›å»ºç»“æœ
            results = []
            for i, (label, score) in enumerate(zip(emotion_labels, scores)):
                results.append({
                    'emotion': label,
                    'score': float(score),
                    'level': int(round(score)),  # 0-3çº§åˆ«
                    'percentage': float(score / 3.0 * 100)  # ç™¾åˆ†æ¯”
                })
            
            return {
                'emotions': results,
                'dominant_emotion': emotion_labels[np.argmax(scores)],
                'processing_time': f"{inference_time*1000:.1f}ms",
                'overall_engagement': float(scores[1]),  # å‚ä¸åº¦
                'overall_confusion': float(scores[2])     # å›°æƒ‘åº¦
            }
            
    except Exception as e:
        logger.error(f"æƒ…ç»ªé¢„æµ‹å¤±è´¥: {e}")
        return None

# åˆå§‹åŒ–è§†é¢‘å¤„ç†å™¨
video_processor = VideoProcessor()

@app.route('/api/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    global model
    
    status = {
        'status': 'healthy' if model is not None else 'unhealthy',
        'service': 'DAiSEE Emotion Recognition API',
        'model_loaded': model is not None,
        'device': str(device) if device else 'unknown',
        'timestamp': datetime.now().isoformat()
    }
    
    return jsonify(status)

@app.route('/api/emotions/analyze', methods=['POST'])
def analyze_emotions():
    """è§†é¢‘æƒ…ç»ªåˆ†ææ¥å£"""
    try:
        start_time = time.time()
        
        # æ£€æŸ¥æ¨¡å‹çŠ¶æ€
        if model is None:
            return jsonify({
                'success': False,
                'error': 'Model not loaded',
                'response': 'æ¨¡å‹æœªåŠ è½½ï¼Œè¯·æ£€æŸ¥æœåŠ¡çŠ¶æ€'
            }), 500
        
        # æ£€æŸ¥è¯·æ±‚æ•°æ®
        if 'video' not in request.files:
            return jsonify({
                'success': False,
                'error': 'No video file',
                'response': 'è¯·ä¸Šä¼ è§†é¢‘æ–‡ä»¶'
            }), 400
        
        video_file = request.files['video']
        if video_file.filename == '':
            return jsonify({
                'success': False,
                'error': 'Empty filename',
                'response': 'æ–‡ä»¶åä¸ºç©º'
            }), 400
        
        # éªŒè¯æ–‡ä»¶ç±»å‹
        allowed_extensions = {'.mp4', '.avi', '.mov', '.mkv', '.webm'}
        file_ext = os.path.splitext(video_file.filename)[1].lower()
        if file_ext not in allowed_extensions:
            return jsonify({
                'success': False,
                'error': 'Invalid file type',
                'response': f'ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_ext}'
            }), 400
        
        # è¯»å–è§†é¢‘æ•°æ®
        video_data = video_file.read()
        logger.info(f"æ¥æ”¶è§†é¢‘æ–‡ä»¶: {video_file.filename}, å¤§å°: {len(video_data)} bytes")
        
        # å¤„ç†è§†é¢‘
        video_tensor = video_processor.extract_frames_from_video(video_data)
        if video_tensor is None:
            return jsonify({
                'success': False,
                'error': 'Video processing failed',
                'response': 'è§†é¢‘å¤„ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥è§†é¢‘æ ¼å¼'
            }), 400
        
        # æƒ…ç»ªé¢„æµ‹
        emotion_results = predict_emotions(video_tensor)
        if emotion_results is None:
            return jsonify({
                'success': False,
                'error': 'Emotion prediction failed',
                'response': 'æƒ…ç»ªé¢„æµ‹å¤±è´¥'
            }), 500
        
        total_time = time.time() - start_time
        
        # è¿”å›ç»“æœ
        response_data = {
            'success': True,
            'data': emotion_results,
            'metadata': {
                'filename': secure_filename(video_file.filename),
                'file_size': len(video_data),
                'total_processing_time': f"{total_time*1000:.1f}ms",
                'frames_processed': len(video_tensor),
                'frame_resolution': video_processor.target_size
            },
            'timestamp': datetime.now().isoformat()
        }
        
        logger.info(f"æƒ…ç»ªåˆ†æå®Œæˆ: {video_file.filename}, ç”¨æ—¶: {total_time*1000:.1f}ms")
        return jsonify(response_data)
        
    except Exception as e:
        logger.error(f"APIé”™è¯¯: {e}")
        return jsonify({
            'success': False,
            'error': str(e),
            'response': 'æœåŠ¡å™¨å†…éƒ¨é”™è¯¯'
        }), 500

@app.route('/api/emotions/test', methods=['POST'])
def test_with_dummy_data():
    """ä½¿ç”¨è™šæ‹Ÿæ•°æ®æµ‹è¯•æ¥å£"""
    try:
        if model is None:
            return jsonify({
                'success': False,
                'error': 'Model not loaded'
            }), 500
        
        # åˆ›å»ºè™šæ‹Ÿè§†é¢‘æ•°æ®
        dummy_video = np.random.rand(16, 224, 224, 3).astype(np.float32)
        
        # é¢„æµ‹æƒ…ç»ª
        emotion_results = predict_emotions(dummy_video)
        
        return jsonify({
            'success': True,
            'data': emotion_results,
            'metadata': {
                'test_mode': True,
                'dummy_data': True
            },
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/models', methods=['GET'])
def get_available_models():
    """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
    models = [
        {
            'id': 'cnn3d-daisee',
            'name': 'CNN3D DAiSEE Model',
            'description': 'åŸºäº3Då·ç§¯ç¥ç»ç½‘ç»œçš„æƒ…ç»ªè¯†åˆ«æ¨¡å‹',
            'emotions': emotion_labels,
            'input_format': 'video',
            'loaded': model is not None
        }
    ]
    
    return jsonify({
        'success': True,
        'models': models,
        'timestamp': datetime.now().isoformat()
    })

if __name__ == '__main__':
    print("="*70)
    print("ğŸš€ å¯åŠ¨DAiSEEæƒ…ç»ªè¯†åˆ«APIæœåŠ¡")
    print("="*70)
    
    # åŠ è½½æ¨¡å‹
    if load_model():
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    else:
        print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥")
    
    print(f"\nğŸ“‹ APIç«¯ç‚¹:")
    print(f"   GET  /api/health           - å¥åº·æ£€æŸ¥")
    print(f"   POST /api/emotions/analyze - å•è§†é¢‘åˆ†æ")
    print(f"   POST /api/emotions/test    - æµ‹è¯•æ¥å£")
    print(f"   GET  /api/models           - æ¨¡å‹åˆ—è¡¨")
    
    print(f"\nğŸŒ æœåŠ¡åœ°å€: http://localhost:5000")
    print(f"ğŸ“± è®¾å¤‡: {device}")
    print("="*70)
    
    # å¯åŠ¨FlaskæœåŠ¡
    app.run(host='0.0.0.0', port=5000, debug=True) 